version: "3"

volumes:
  temp: {}

services:
  llama-bot:
    container_name: llama-telegram-bot
    restart: always
    build:
      context: "."
      dockerfile: Dockerfile.cuda
    volumes:
      - /path/to/model:/models
      - temp:/tmp
    environment:
      BOT_TOKEN: <add bot token here>
      MODEL_PATH: /models/Wizard-Vicuna-7B-Uncensored.ggmlv3.q4_0.bin
      ALLOWED_USERS: ""
      GPU_LAYERS: 32

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
