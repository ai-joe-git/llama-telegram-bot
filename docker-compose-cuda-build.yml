version: "3"

services:
  llama-bot:
    container_name: llama-telegram-bot
    restart: always
    build:
      context: "."
      dockerfile: Dockerfile.cuda
    volumes:
      - /path/to/models:/models
    environment:
      BOT_TOKEN: <Add your telegram bot token here>
      MODEL_PATH: /models/wizardLM-7B.ggml.q4_0.bin
      ALLOWED_USERS: ""
      GPU_LAYERS: 26

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]